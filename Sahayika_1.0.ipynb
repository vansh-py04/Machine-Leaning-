{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.txt','r',errors='ignore')\n",
    "raw_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.lower()\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "sen_token = nltk.sent_tokenize(raw_data)\n",
    "word_token = nltk.word_tokenize(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Text Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def Lemtokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def lemNormalize(text):\n",
    "    return Lemtokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greeting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = ('hello','hi','help','sahayika','hii')\n",
    "# response = ('Hi there, I am Sahayika your virtual legal assistant. How may I help you?')\n",
    "def greet(sen):\n",
    "    for word in sen.split():\n",
    "        if word.lower() in greeting:\n",
    "            return 'Hi there, I am Sahayika your virtual legal assistant. How may I help you?'\n",
    "            # return random.choice(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    Tfidfvec = TfidfVectorizer(tokenizer=lemNormalize,stop_words='english')\n",
    "    tfidf = Tfidfvec.fit_transform(sen_token)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response = robo_response + \"I am sorry, I am unable to understand\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response + sen_token[idx]\n",
    "        return robo_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a Learning Bot. For ending type bye\n",
      "Bot: Hi there, I am Sahayika your virtual legal assistant. How may I help you?\n",
      "company law\n",
      "main article: indian company law\n",
      "the current indian company law was updated and recodified in the companies act 2013.\n",
      "\n",
      "tort law\n",
      "main article: tort law in india\n",
      "tort law in india is primarily governed by judicial precedent as in other common law jurisdictions, supplemented by statutes governing damages, civil procedure, and codifying common law torts.\n",
      "Bot : Goodbye\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, I am a Learning Bot. For ending type bye\")\n",
    "flag = True\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thank you' or user_response=='thanks'):\n",
    "            flag = False\n",
    "            print(\"Bot: You are Welcome\")\n",
    "        else:\n",
    "            if(greet(user_response) != None):\n",
    "                print('Bot:',greet(user_response))\n",
    "            else:\n",
    "                sen_token.append(user_response)\n",
    "                word_token = word_token + nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_token))\n",
    "                print(response(user_response))\n",
    "                sen_token.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Bot : Goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
